## **Sign Language Recognition for Emergency Communication**  
ğŸš€ **Real-time AI-powered sign language recognition system for emergency communication using deep learning.**  

---

### ğŸ“Œ **Project Overview**  
This project aims to assist the **deaf and hard-of-hearing community** by recognizing **emergency sign language gestures** using **computer vision and deep learning**. The system captures hand gestures via a **webcam**, classifies them using a **CNN-based MobileNetV2 model**, and provides an **audio alert** for communication with emergency services.

### âœ… **Key Features**  
âœ” **Real-time sign language recognition** using OpenCV & PyTorch  
âœ” **Custom dataset** of 8 emergency signs  
âœ” **Deep learning model (MobileNetV2)** trained with transfer learning  
âœ” **Temporal analysis (majority voting over frames)** for better accuracy  
âœ” **Text-to-speech (TTS)** for immediate audio alerts  
âœ” **Future scope: Mobile app deployment with TensorFlow Lite**  

---

## ğŸ“‚ **Dataset & Classes**  
This project uses **custom sign language videos**, converted into frames and labeled into **8 emergency-related classes**:  

ğŸ”´ **Danger**  
ğŸ†˜ **Emergency**  
ğŸ”¥ **Fire**  
ğŸ¤ **Help**  
ğŸ™ **Please**  
ğŸš” **Police**  
âœ‹ **Stop**  
ğŸ’§ **Water**  

Each class contains **500 images**, balanced through **data augmentation**.

---

## ğŸ›  **Technologies Used**  
- **Python** (Core Programming)  
- **OpenCV** (Computer Vision & Webcam Integration)  
- **PyTorch & Torchvision** (Deep Learning)  
- **Mediapipe** (Hand Tracking & ROI Detection)  
- **Pyttsx3 & gTTS** (Text-to-Speech for Audio Alerts)  
- **Google Colab / Jupyter Notebook** (Model Training)  

---

## ğŸš€ **Installation & Setup**  
### ğŸ”¹ **1. Clone the Repository**  
```bash
git clone https://github.com/your-username/sign-language-recognition.git
cd sign-language-recognition
```

### ğŸ”¹ **2. Install Dependencies**  
```bash
pip install -r requirements.txt
```

### ğŸ”¹ **3. Run the Webcam-Based Gesture Recognition**  
```bash
python sign_language_detection.py
```

### ğŸ”¹ **4. Train the Model (If Needed)**  
```bash
python train_model.py
```

---

## ğŸ–¼ **Preprocessing Pipeline**  
1ï¸âƒ£ **Frame Extraction:** Convert videos into images at **30 FPS**.  
2ï¸âƒ£ **Hand Detection:** Use **Mediapipe** to **crop the hand region**.  
3ï¸âƒ£ **Preprocessing:** Resize (224Ã—224), Normalize, Grayscale, Edge Detection.  
4ï¸âƒ£ **Data Augmentation:** Flip, Rotate, Scale to **balance dataset**.  
5ï¸âƒ£ **Train CNN Model (MobileNetV2)** using **Transfer Learning**.  
6ï¸âƒ£ **Deploy Model in Real-Time** using **OpenCV & PyTorch**.  
7ï¸âƒ£ **Use Majority Voting over 30 Frames** for **better accuracy**.  
8ï¸âƒ£ **Convert Predictions to Speech** using **pyttsx3 / gTTS**.  

---

## ğŸ¯ **Real-Time Gesture Detection & Audio Alerts**  
Once deployed, the system:  
âœ” Captures **live video feed** from the **webcam**  
âœ” Predicts sign gestures using the **trained MobileNetV2 model**  
âœ” Uses **majority voting (over 30 frames)** to stabilize predictions  
âœ” Converts sign language to **speech output** for emergency alerts  

ğŸ’¡ Example: If a user signs **â€œHELPâ€**, the system will:  
ğŸ–¥ Display: `"Help detected!"`  
ğŸ”Š Speak: `"Help! Emergency detected!"`  

---

## ğŸ“Š **Model Training & Performance**  
âœ… **Trained on a balanced dataset (500 images per class)**  
âœ… **Achieved ~90% accuracy** on the test set  
âœ… **Used Adam Optimizer, Cross-Entropy Loss, 10 epochs**  
âœ… **Confusion matrix analysis** to minimize misclassifications  

---

## ğŸ“Œ **Future Improvements**  
âœ… **Deploy on Mobile** using **TensorFlow Lite**  
âœ… **Support More Sign Languages** (ASL, ISL, etc.)  
âœ… **Improve Gesture Recognition with LSTMs** (Sequence Learning)  
âœ… **Enable Cloud-Based Emergency Alerting**  

---

## ğŸ‘©â€ğŸ’» **Contributions**  
ğŸ’¡ Want to improve this project? Follow these steps:  

1. **Fork the repository**  
2. **Create a new branch** (`git checkout -b feature-branch`)  
3. **Commit changes** (`git commit -m "Added new feature"`)  
4. **Push to GitHub** (`git push origin feature-branch`)  
5. **Submit a Pull Request** ğŸ‰  

---

## ğŸ“œ **License**  
This project is **open-source** under the **MIT License**.  

---

### ğŸš€ **Maintainer & Contact**  
ğŸ”— **GitHub:** [Your GitHub Username](https://github.com/tanishaagarwal195)  
ğŸ“§ **Email:** tanishaagarwal400@gmail.com 

---

### ğŸ¯ **Final Thoughts**  
ğŸš€ This project bridges the **communication gap** for the **deaf community** in **emergencies** by combining **AI, computer vision, and deep learning**.  

ğŸ‘€ **Try it, contribute, and help make communication more accessible!**  

---
