{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT6PU8u9rXrz"
      },
      "source": [
        "## **Domain**\n",
        "\n",
        "This project operates within the domain of Sign Language Recognition for Emergency Response Systems, an intersection of computer vision, gesture recognition, and assistive technology. The system leverages real-time hand gesture detection and classification to enable effective communication in high-stakes situations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG46tQXXllhc"
      },
      "source": [
        "# PROJECT : SIGN LANGUAGE RECOGNITION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uPwjGXlrXr0"
      },
      "source": [
        "## **Problem Statement**\n",
        "\n",
        "The primary objective of this dataset is to enable the development of a sign language recognition system that assists the deaf and hard-of-hearing community in emergency scenarios. The system should:\n",
        "- Recognize specific hand gestures captured via a smartphone camera.\n",
        "- Map these gestures to predefined emergency classes such as \"Danger,\" \"Help,\" and \"Police.\"\n",
        "- Facilitate non-verbal communication by triggering automated alerts to relevant authorities or services."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxY4aEhUuCDr"
      },
      "source": [
        "## **Real-Life Use Case**\n",
        "\n",
        "### Mobile Emergency Application\n",
        "An application that integrates real-time sign language recognition, allowing individuals to communicate during emergencies. The app will:\n",
        "- Recognize specific gestures related to emergency situations.\n",
        "- Trigger notifications to relevant emergency services.\n",
        "\n",
        "**Example Scenario**\n",
        "A deaf person trapped in a building during a fire performs the \"Fire\" gesture in front of their smartphone camera. The app detects the gesture and sends an alert to the fire department, including the personâ€™s location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heyjnyUXuEGe"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqPkv_OhrXr0"
      },
      "source": [
        "## **Dataset Description**\n",
        "\n",
        "The dataset is designed to support the development of a mobile emergency application for sign language recognition. It contains images and videos representing 8 distinct emergency-related gestures, ensuring inclusivity and practicality for individuals who are deaf or hard of hearing. The dataset emphasizes diverse environmental conditions to enhance model robustness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB4d0ybirXr1"
      },
      "source": [
        "## **Classes**\n",
        "\n",
        "The dataset includes the following classes:\n",
        "\n",
        "- **Danger** : Indicates a potential threat or hazard.\n",
        "- **Help**: Requests immediate assistance.\n",
        "- **Stop**: Signals a halt or warning.\n",
        "- **Please**: Indicates the urge to ask someone to help them do something.\n",
        "- **Fire**: Communicates the presence of a fire.\n",
        "- **Water**: Represents the need for water in emergency situations.\n",
        "- **Police**: Indicates the need for law enforcement.\n",
        "- **Emergency**: Represents a general state of urgency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2lbTorbrXr2"
      },
      "source": [
        "## **Data Collection**\n",
        "- Collect video frames of 8 emergency gestures performed by diverse individuals.\n",
        "- Ensure varied hand orientations, lighting conditions, and backgrounds to improve robustness.\n",
        "- Include diverse demographics to account for variations in hand size and shape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtkU_WI4uLXv"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH0IcGEqrXr3"
      },
      "source": [
        "## **Dataset Preparation Steps**\n",
        "\n",
        "- *Data Unzipping*\n",
        "    - Extract raw data from compressed files.\n",
        "- *Frame Extraction*\n",
        "    - Convert video data into individual frames for analysis.\n",
        "- *Upsampling*\n",
        "    - Balance the dataset by increasing the number of underrepresented class samples.\n",
        "- *Preprocessing Frames* with Hand Detection and Cropping (ROI Extraction)\n",
        "    - Extract Region of Interest (**ROI**) focusing on hand regions.\n",
        "    - **Resize** images to a consistent dimension (e.g., 224x224 pixels).\n",
        "    - **Normalize** pixel values to enhance model performance.\n",
        "    - Convert images to **grayscale** for simplicity.\n",
        "    - Apply **Canny edge detection** for emphasizing hand contours.\n",
        "\n",
        "- *Balanced Augmentation*\n",
        "    - Apply rotation, flipping, scaling, and synthetic noise to simulate diverse conditions and improve robustness.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O0xVlU9rXr3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKQAhW4jrXr3"
      },
      "source": [
        "## PART 1 : PRE-PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnUYXCrPlCbV",
        "outputId": "2d7059ac-cf97-4fb7-807b-af0fbccff6fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeCdz0hHbSwG"
      },
      "source": [
        "### **Unzipping the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5psm4GwlYOM",
        "outputId": "1f27f67b-0a8a-4eb4-8ac0-2692e1a9f5fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset unzipped!\n"
          ]
        }
      ],
      "source": [
        "import zipfile #  Provides tools for working with `.zip` files.\n",
        "import os # Used for interacting with the file system\n",
        "\n",
        "# Specify the path to the zip file containing the dataset on Google Drive\n",
        "zip_path = '/content/drive/MyDrive/Sign Language Recognition/Dataset.zip'\n",
        "\n",
        "# Specify the directory where the extracted files will be saved\n",
        "extract_path = '/content/drive/MyDrive/Sign Language Recognition/'\n",
        "\n",
        "# Open the zip file in read mode using the zipfile module\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    # Extract all the contents of the zip file to the specified directory\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# Print a confirmation message after successful extraction\n",
        "print(\"Dataset unzipped!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hCn3mdul9N0"
      },
      "source": [
        "### **Extracting Frames from Videos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulZ3hyYarXr6"
      },
      "source": [
        "This script extracts frames from videos stored in a structured folder, organized by classes, at a frame rate of 30 FPS. The extracted frames are saved in a corresponding structured output folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETqv_Eq4l1R6",
        "outputId": "24363e99-2d99-4510-e012-b54caa6e6b05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frames extracted!\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import glob\n",
        "\n",
        "def extract_frames_from_videos(input_folder, output_folder, fps=30):\n",
        "    # Create the output folder if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Iterate through all class folders in the input folder\n",
        "    for class_folder in os.listdir(input_folder):\n",
        "        class_path = os.path.join(input_folder, class_folder)  # Path to the class folder\n",
        "        output_class_folder = os.path.join(output_folder, class_folder)  # Corresponding output folder\n",
        "        os.makedirs(output_class_folder, exist_ok=True)  # Create output folder for the class\n",
        "\n",
        "        # Process all video files in the current class folder\n",
        "        for video_file in glob.glob(f\"{class_path}/*.mp4\"):\n",
        "            cap = cv2.VideoCapture(video_file)  # Open the video file\n",
        "            video_name = os.path.splitext(os.path.basename(video_file))[0]  # Get video name without extension\n",
        "            frame_rate = int(cap.get(cv2.CAP_PROP_FPS))  # Get the video frame rate\n",
        "            frame_interval = max(1, frame_rate // fps)  # Calculate interval for extracting frames\n",
        "            frame_count = 0  # Count of frames processed\n",
        "            frame_index = 0  # Index for extracted frames\n",
        "\n",
        "            # Read frames from the video\n",
        "            while cap.isOpened():\n",
        "                ret, frame = cap.read()  # Read the next frame\n",
        "                if not ret:  # Break the loop if no more frames\n",
        "                    break\n",
        "                if frame_count % frame_interval == 0:  # Check if the frame should be saved\n",
        "                    frame_name = f\"{output_class_folder}/{video_name}_frame_{frame_index}.jpg\"  # Frame file name\n",
        "                    cv2.imwrite(frame_name, frame)  # Save the frame as an image\n",
        "                    frame_index += 1  # Increment frame index\n",
        "                frame_count += 1  # Increment frame count\n",
        "            cap.release()  # Release the video capture object\n",
        "    print(\"Frames extracted!\")  # Confirmation message\n",
        "\n",
        "# Paths for input videos and output frames\n",
        "input_videos_path = '/content/drive/MyDrive/Sign Language Recognition/Dataset'\n",
        "output_frames_path = '/content/drive/MyDrive/Sign Language Recognition/Frame_extracted_data'\n",
        "\n",
        "# Extract frames at 30 FPS\n",
        "extract_frames_from_videos(input_videos_path, output_frames_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s487qJYgrXr7"
      },
      "source": [
        "### **Counting Frames per Class**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NVP3pQ6rXr7"
      },
      "source": [
        "This script calculates the number of extracted frames for each class in the specified folder. The results are returned as a dictionary, where the keys are class names and the values are the corresponding frame counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKMZl8LemQuW",
        "outputId": "3b8a77ae-2c40-48cd-861d-9175e956360e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frame counts per class: {'danger': 432, 'emergency': 273, 'fire': 180, 'help': 604, 'please': 523, 'police': 496, 'stop': 304, 'water': 676}\n"
          ]
        }
      ],
      "source": [
        "def count_frames_per_class(folder_path):\n",
        "    frame_counts = {}  # Dictionary to store the count of frames for each class\n",
        "\n",
        "    # Iterate through each class folder in the specified folder path\n",
        "    for class_folder in os.listdir(folder_path):\n",
        "        class_path = os.path.join(folder_path, class_folder)  # Path to the current class folder\n",
        "\n",
        "        # Count the number of .jpg files (frames) in the class folder\n",
        "        frame_counts[class_folder] = len(glob.glob(f\"{class_path}/*.jpg\"))\n",
        "\n",
        "    # Return the dictionary containing frame counts for each class\n",
        "    return frame_counts\n",
        "\n",
        "# Call the function to count frames in the extracted frames folder\n",
        "frame_counts = count_frames_per_class(output_frames_path)\n",
        "\n",
        "# Print the frame counts for each class\n",
        "print(\"Frame counts per class:\", frame_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VpyThByrXr7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C7OaPHRT_78"
      },
      "source": [
        "###  **Upsampling Classes with Augmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jymOntIMrXr8"
      },
      "source": [
        "This script balances the dataset by upsampling classes with fewer frames. It uses specified augmentations to generate new frames and ensures all classes have the same number of frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztkoelV7RRnU",
        "outputId": "14430bcb-4f91-4586-951e-c1f672d20715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upsampling with specified augmentations complete!\n",
            "Updated frame counts per class: {'danger': 676, 'emergency': 676, 'fire': 676, 'help': 676, 'please': 676, 'police': 676, 'stop': 676, 'water': 676}\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def augment_frame(frame):\n",
        "    # Apply one of the specified augmentations to the frame\n",
        "    augmentations = [\n",
        "        lambda x: cv2.flip(x, 1),  # Horizontal flip\n",
        "        lambda x: cv2.rotate(x, cv2.ROTATE_90_CLOCKWISE),  # Rotate 90 degrees clockwise\n",
        "        lambda x: cv2.rotate(x, cv2.ROTATE_90_COUNTERCLOCKWISE),  # Rotate 90 degrees counterclockwise\n",
        "    ]\n",
        "    # Randomly select and apply an augmentation\n",
        "    augmentation = random.choice(augmentations)\n",
        "    return augmentation(frame)\n",
        "\n",
        "def upsample_classes_with_augmentation(folder_path, frame_counts):\n",
        "    # Determine the maximum number of frames among all classes\n",
        "    max_frames = max(frame_counts.values())\n",
        "\n",
        "    # Iterate through each class and its frame count\n",
        "    for class_folder, count in frame_counts.items():\n",
        "        class_path = os.path.join(folder_path, class_folder)  # Path to the class folder\n",
        "        if count < max_frames:\n",
        "            # Get a list of all existing frames in the class folder\n",
        "            current_files = glob.glob(f\"{class_path}/*.jpg\")\n",
        "            for i in range(max_frames - count):  # Calculate how many frames need to be added\n",
        "                # Choose a random frame from the existing files\n",
        "                source_file = random.choice(current_files)\n",
        "                frame = cv2.imread(source_file)  # Read the selected frame\n",
        "\n",
        "                # Apply augmentation to create a new frame\n",
        "                augmented_frame = augment_frame(frame)\n",
        "\n",
        "                # Save the augmented frame with a new name\n",
        "                target_file = f\"{class_path}/aug_{i}.jpg\"\n",
        "                cv2.imwrite(target_file, augmented_frame)\n",
        "    print(\"Upsampling with specified augmentations complete!\")\n",
        "\n",
        "# Perform upsampling using the function\n",
        "upsample_classes_with_augmentation(output_frames_path, frame_counts)\n",
        "\n",
        "# Recount the frames in each class after upsampling\n",
        "updated_frame_counts = count_frames_per_class(output_frames_path)\n",
        "\n",
        "# Print the updated frame counts\n",
        "print(\"Updated frame counts per class:\", updated_frame_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LRR-ejirXr8"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
